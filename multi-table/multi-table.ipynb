{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uUvsR-mWBoNS"
   },
   "source": [
    "# Perform Multi-Table Synthesis\n",
    "\n",
    "In this exercise, we are going to walk through the synthesis of a relational table structure. For that, we will be using a slightly trimmed down version of the Berka dataset [[1](#refs)]. It consists of a total of 8 tables, with one of these (\"district\") serving as a reference table, and all others containing privacy-sensitive information.\n",
    "\n",
    "<img src='./berka-original.png' width=\"600px\"/>\n",
    "\n",
    "At the time of writing, MOSTLY AI requires data for a multi-table scenario to be provisioned via a relational database, and also to be delivered to a relational database. Thus we will first provide helper scripts for creating two public database instances, to load the original data into one of them, and to then make the required job configuration.\n",
    "\n",
    "Once synthesized, we will check for referential integrity, as well as for the retention of specific statistical properties that span multiple tables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oZ7ERZK__8TB",
    "tags": []
   },
   "source": [
    "## Import Data to a Database\n",
    "\n",
    "If you don't have a DB server available, then go to your preferred cloud provider (AWS, GPC, Azure, etc.) and launch an instance there first. Make sure that clients can connect externally via username / password credentials, and have the required rights to create, update and delete database instances there.\n",
    "\n",
    "<img src='./sql1.png' width=\"400px\"/> <img src='./sql2.png' width=\"400px\"/><br /><img src='./sql3.png' width=\"400px\"/> <img src='./sql4.png' width=\"400px\"/>\n",
    "\n",
    "Once in place, please update the following variables accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "db_host = \"104.155.103.36\"\n",
    "db_usr = \"postgres\"\n",
    "db_pwd = \"hello-world\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's then create two database instances. One, that will contain the original data. And another one, that will serve us as a destination for the synthetic tables.\n",
    "\n",
    "For that we will need to install SQLAlchemy 2.x."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # install required Python packages\n",
    "# !pip install --pre psycopg2 sqlalchemy==2.0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sqlalchemy\n",
    "import psycopg2\n",
    "from sqlalchemy import text\n",
    "from sqlalchemy import create_engine\n",
    "print(f\"SQLAlchemy v{sqlalchemy.__version__}\")\n",
    "assert sqlalchemy.__version__.startswith('2.')\n",
    "\n",
    "def create_db(host, user, pwd, db_name, if_exists=\"fail\"):\n",
    "    con = psycopg2.connect(f\"postgresql://{user}:{pwd}@{host}:5432/postgres\")\n",
    "    con.autocommit = True\n",
    "    cur = con.cursor()\n",
    "    cur.execute(f\"SELECT 1 FROM pg_catalog.pg_database WHERE datname = '{db_name}'\")\n",
    "    exists = cur.fetchone()\n",
    "    if exists and if_exists == \"fail\":\n",
    "        raise Exception(f\"database {db_name} already exists\")\n",
    "    elif exists and if_exists == \"replace\":\n",
    "        cur.execute(\"DROP DATABASE \" + db_name)\n",
    "    cur.execute(\"CREATE DATABASE \" + db_name)\n",
    "    con.close()\n",
    "\n",
    "def connect_db(host, user, pwd, db_name):\n",
    "    engine = create_engine(f\"postgresql://{user}:{pwd}@{host}:5432/{db_name}\")\n",
    "    return engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Source and Destination Database\n",
    "\n",
    "replace `if_exists='replace'` if you want to re-create the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "db_name_source = 'berka_original'\n",
    "create_db(db_host, db_usr, db_pwd, db_name_source, if_exists=\"replace\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_name_destination = 'berka_synthetic'\n",
    "create_db(db_host, db_usr, db_pwd, db_name_destination, if_exists=\"replace\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data into Source Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# check whether we are in Google colab\n",
    "try:\n",
    "    from google.colab import files\n",
    "    print(\"running in COLAB mode\")\n",
    "    repo = 'https://github.com/mostly-ai/mostly-tutorials/raw/dev/multi-table'\n",
    "except:\n",
    "    print(\"running in LOCAL mode\")\n",
    "    repo = '.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import data into DB\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "csv_files = [\n",
    "    f'{repo}/account.csv', \n",
    "    f'{repo}/card.csv', \n",
    "    f'{repo}/client.csv', \n",
    "    f'{repo}/disposition.csv', \n",
    "    f'{repo}/district.csv',\n",
    "    f'{repo}/loan.csv', \n",
    "    f'{repo}/orders.csv', \n",
    "    f'{repo}/transaction.csv'\n",
    "]\n",
    "\n",
    "engine = connect_db(db_host, db_usr, db_pwd, db_name_source)\n",
    "\n",
    "originals = {}\n",
    "for fn in csv_files:\n",
    "    # read data from CSV into Pandas DataFrame\n",
    "    df = pd.read_csv(fn)\n",
    "    # ensure all columns are NULL-able\n",
    "    df = df.convert_dtypes()\n",
    "    # convert date columns\n",
    "    for col in df.columns:\n",
    "        if col in ['date', 'issued']:\n",
    "            df[col] = pd.to_datetime(df[col])\n",
    "        if col.endswith('_id'):\n",
    "            df[col] = df[col].astype(str)\n",
    "    # get filename w/o extension\n",
    "    db_table = Path(fn).stem\n",
    "    # write DataFrame to DB\n",
    "    df.to_sql(db_table, engine, index=False, if_exists='fail')\n",
    "    print(f\"created table `{db_table}` with {df.shape[0]:,} records\")\n",
    "    originals[db_table] = df\n",
    "\n",
    "print('DONE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with engine.connect() as conn:\n",
    "    # define primary keys in the database\n",
    "    conn.execute(text('ALTER TABLE account ADD PRIMARY KEY (account_id);'))\n",
    "    conn.execute(text('ALTER TABLE card ADD PRIMARY KEY (card_id);'))\n",
    "    conn.execute(text('ALTER TABLE client ADD PRIMARY KEY (client_id);'))\n",
    "    conn.execute(text('ALTER TABLE disposition ADD PRIMARY KEY (disposition_id);'))\n",
    "    conn.execute(text('ALTER TABLE district ADD PRIMARY KEY (district_id);'))\n",
    "    conn.execute(text('ALTER TABLE loan ADD PRIMARY KEY (loan_id);'))\n",
    "    conn.execute(text('ALTER TABLE orders ADD PRIMARY KEY (orders_id);'))\n",
    "    conn.execute(text('ALTER TABLE transaction ADD PRIMARY KEY (transaction_id);'))\n",
    "    print(f\"created primary keys\")\n",
    "    # define foreign key constraints in the database\n",
    "    conn.execute(text('ALTER TABLE account ADD CONSTRAINT fk_district_a FOREIGN KEY (district_id) REFERENCES district (district_id);')) #\n",
    "    conn.execute(text('ALTER TABLE client ADD CONSTRAINT fk_district_c FOREIGN KEY (district_id) REFERENCES district (district_id);')) #\n",
    "    conn.execute(text('ALTER TABLE disposition ADD CONSTRAINT fk_disp_a FOREIGN KEY (account_id) REFERENCES account (account_id);'))\n",
    "    conn.execute(text('ALTER TABLE disposition ADD CONSTRAINT fk_disp_c FOREIGN KEY (client_id) REFERENCES client (client_id);'))\n",
    "    conn.execute(text('ALTER TABLE card ADD CONSTRAINT fk_card FOREIGN KEY (disposition_id) REFERENCES disposition (disposition_id);'))\n",
    "    conn.execute(text('ALTER TABLE transaction ADD CONSTRAINT fk_trans FOREIGN KEY (account_id) REFERENCES account (account_id);'))\n",
    "    conn.execute(text('ALTER TABLE loan ADD CONSTRAINT fk_loan FOREIGN KEY (account_id) REFERENCES account (account_id);'))\n",
    "    conn.execute(text('ALTER TABLE orders ADD CONSTRAINT fk_order FOREIGN KEY (account_id) REFERENCES account (account_id);'))\n",
    "    print(f\"created foreign keys\")\n",
    "    conn.commit()\n",
    "print('DONE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oZ7ERZK__8TB",
    "tags": []
   },
   "source": [
    "## Synthesize Data via MOSTLY AI\n",
    "\n",
    "Go to MOSTLY AI, and\n",
    "\n",
    "1. Create two data connectors, one for the source DB `berka_original`, and one for the destination DB `berka_synthetic`\n",
    "\n",
    "2. Create a data catalog using the data connector for `berka_original`\n",
    "\n",
    "    - Select all 8 tables for the data catalog\n",
    "    - Select `client` and `account` as subject tables\n",
    "    - Keep the ranking of the subject table as-is\n",
    "    - Configure smart select column `district_id` for the `disposition -> client` relation\n",
    "\n",
    "<img src='./mostly1.png' width=\"400px\"/> <img src='./mostly2.png' width=\"400px\"/><br />\n",
    "<img src='./mostly3.png' width=\"400px\"/> <img src='./mostly4.png' width=\"400px\"/><br />\n",
    "<img src='./mostly5.png' width=\"400px\"/> <img src='./mostly6.png' width=\"400px\"/><br />\n",
    "\n",
    "These are then the configured table types and relations.\n",
    "\n",
    "<img src='./berka-synthetic.png' width=\"600px\"/>\n",
    "\n",
    "3. Launch the job, and select `berka_synthetic` as a destination in \"Output settings\"\n",
    "\n",
    "4. Once the job has completed, continue with executing the next cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# fetch synthetic data from destination database\n",
    "engine = connect_db(db_host, db_usr, db_pwd, db_name_destination)\n",
    "tables = [Path(fn).stem for fn in csv_files if 'district' not in fn]\n",
    "synthetics = {}\n",
    "for db_table in tables:\n",
    "    with engine.begin() as conn:\n",
    "        df = pd.read_sql_query(sql=text(f'select * from {db_table};'), con=conn)\n",
    "    print(f\"extracted table {db_table} with {df.shape[0]:,} records\")\n",
    "    synthetics[db_table] = df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore Synthetic Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Show sample records for each table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for k in synthetics:\n",
    "    print(\"===\", k, \"===\")\n",
    "    display(synthetics[k].sample(n=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Check basic statistics\n",
    "\n",
    "The newly generated tables are statistically representative of the original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "display(synthetics['transaction']['amount'].quantile(q=[.1, .5, .9]))\n",
    "display(originals['transaction']['amount'].quantile(q=[.1, .5, .9]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "display(synthetics['account']['date'].quantile(q=[.1, .5, .9], interpolation='nearest'))\n",
    "display(pd.to_datetime(originals['account']['date']).quantile(q=[.1, .5, .9], interpolation='nearest'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Check referential integrity\n",
    "\n",
    "The newly generated foreign keys are also present as primary keys in the connected tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "assert synthetics['transaction']['account_id'].isin(synthetics['account']['account_id']).all()\n",
    "assert synthetics['card']['disposition_id'].isin(synthetics['disposition']['disposition_id']).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check context relations\n",
    "\n",
    "The cardinality of context FK relations is perfectly retained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('Orders per Account - Synthetic')\n",
    "display(synthetics['orders'].groupby('account_id').size().value_counts())\n",
    "print('\\nOrders per Account - Original')\n",
    "display(originals['orders'].groupby('account_id').size().value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('Cards per Disposition - Synthetic')\n",
    "display(synthetics['card'].groupby('disposition_id').size().value_counts())\n",
    "print('\\nCards per Disposition - Original')\n",
    "display(originals['card'].groupby('disposition_id').size().value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check smart select relations\n",
    "\n",
    "The cardinality of smart select FK relation is not retained, as these get randomly assigned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('\\nDispositions per Client - Synthetic')\n",
    "display(synthetics['disposition'].groupby('client_id').size().value_counts())\n",
    "print('Dispositions per Client - Original')\n",
    "display(originals['disposition'].groupby('client_id').size().value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the statistical relations between a child and its randomly assigned smart select parent can be retained, if corresponding smart select columns were configured. E.g. if smart select is properly configured, then the the share of cases where the `client` has the same `district_id` as the `account`, that she owns, should be similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def matching_districts(datasets):    \n",
    "    df = datasets['disposition']\n",
    "    df = df.loc[df.type=='OWNER']\n",
    "    df = df.merge(\n",
    "        datasets['client'], \n",
    "        on='client_id',\n",
    "    ).merge(\n",
    "        datasets['account'], \n",
    "        on='account_id',\n",
    "    )\n",
    "    return (df['district_id_x']==df['district_id_y']).mean()\n",
    "\n",
    "print(f\"Share of accounts and clients with identical district_id\")\n",
    "print(f\"synthetic: {matching_districts(synthetics):4.0%}\")\n",
    "print(f\"original:  {matching_districts(originals):4.0%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UMGNussThvys"
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "In this tutorial we have demonstrated how to synthesize a multi-table relational database. We have seen, that structure, statistics and referential integrity are perfectly retained. We have also seen, how to configure Smart Select, and its impact on retaining statistcs across non-context relations. But we have also seen, that there are limitations to what can be retained, in particular when it comes to the cardinality of smart select relations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References<a class=\"anchor\" name=\"refs\"></a>\n",
    "\n",
    "1. https://data.world/lpetrocelli/czech-financial-dataset-real-anonymized-transactions"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
