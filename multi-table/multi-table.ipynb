{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uUvsR-mWBoNS"
   },
   "source": [
    "# Perform Multi-Table Synthesization\n",
    "\n",
    "In this exercise, we are going to walk through the synthesis of a relational table structure. For that, we will be using a slightly trimmed down version of the Berka dataset [[1](#refs)]: a dataset containing Czech bank transactions. It consists of a total of 8 tables, with one of these (\"district\") serving as a reference table, and all others containing privacy-sensitive information.\n",
    "\n",
    "<img src='https://raw.githubusercontent.com/mostly-ai/mostly-tutorials/dev/multi-table/berka-original.png' width=\"600px\"/>\n",
    "\n",
    "There are two ways to perform multi-table synthesization with MOSTLY AI:\n",
    "1. via an **ad-hoc job** by manually uploading data files (such as CSV) and defining the relationship between the tables.\n",
    "2. by **connecting to a relational database** and importing the relationships from the db directly.\n",
    "\n",
    "You will explore both approaches in this tutorial. We will focus especially on the database connector approach as it is the most commonly used approach for working with multiple tables. To help with setting up the database infrastructure, the tutorial will first provide helper scripts for creating two public database instances, to load the original data into one of them, and to then make the required job configuration.\n",
    "\n",
    "Once the multi-table data has been synthesized, you will check the synthetic data for referential integrity, as well as for the retention of specific statistical properties that span multiple tables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oZ7ERZK__8TB",
    "tags": []
   },
   "source": [
    "## Import Data to a Database\n",
    "\n",
    "If you don't have a DB server available, then go to your preferred cloud provider (AWS, GPC, Azure, etc.) and launch an instance there first. Make sure that clients can connect externally via username / password credentials and have the required rights to create, update and delete database instances there.\n",
    "\n",
    "<img src='https://raw.githubusercontent.com/mostly-ai/mostly-tutorials/dev/multi-table/sql1.png' width=\"400px\"/> <img src='https://raw.githubusercontent.com/mostly-ai/mostly-tutorials/dev/multi-table/sql2.png' width=\"400px\"/><br /><img src='https://raw.githubusercontent.com/mostly-ai/mostly-tutorials/dev/multi-table/sql3.png' width=\"400px\"/> <img src='https://raw.githubusercontent.com/mostly-ai/mostly-tutorials/dev/multi-table/sql4.png' width=\"400px\"/>\n",
    "\n",
    "Once in place, please update the following variables accordingly. For security, we have stored the password in an environment variable. You can of course use your own preferred method for passing the credentials securely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "db_host = \"34.122.91.200\"\n",
    "db_usr = \"postgres\"\n",
    "db_pwd = os.environ[\"DB_PASS\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's then create two database instances:\n",
    "1. an instance that will contain the original data, and \n",
    "2. another instance that will serve as a destination for the synthetic tables.\n",
    "\n",
    "For that we will need to install SQLAlchemy 2.x."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # install required Python packages\n",
    "# !pip install --pre psycopg2-binary sqlalchemy==2.0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sqlalchemy\n",
    "import psycopg2\n",
    "from sqlalchemy import text\n",
    "from sqlalchemy import create_engine\n",
    "print(f\"SQLAlchemy v{sqlalchemy.__version__}\")\n",
    "assert sqlalchemy.__version__.startswith('2.')\n",
    "\n",
    "def create_db(host, user, pwd, db_name, if_exists=\"fail\"):\n",
    "    con = psycopg2.connect(f\"postgresql://{user}:{pwd}@{host}:5432/postgres\")\n",
    "    con.autocommit = True\n",
    "    cur = con.cursor()\n",
    "    cur.execute(f\"SELECT 1 FROM pg_catalog.pg_database WHERE datname = '{db_name}'\")\n",
    "    exists = cur.fetchone()\n",
    "    if exists and if_exists == \"fail\":\n",
    "        raise Exception(f\"database {db_name} already exists\")\n",
    "    elif exists and if_exists == \"replace\":\n",
    "        cur.execute(\"DROP DATABASE \" + db_name)\n",
    "    cur.execute(\"CREATE DATABASE \" + db_name)\n",
    "    con.close()\n",
    "\n",
    "def connect_db(host, user, pwd, db_name):\n",
    "    engine = create_engine(f\"postgresql://{user}:{pwd}@{host}:5432/{db_name}\")\n",
    "    return engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Source and Destination Database\n",
    "\n",
    "replace `if_exists='replace'` if you want to re-create the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "db_name_source = 'berka_original'\n",
    "create_db(db_host, db_usr, db_pwd, db_name_source, if_exists=\"replace\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_name_destination = 'berka_synthetic'\n",
    "create_db(db_host, db_usr, db_pwd, db_name_destination, if_exists=\"replace\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data into Source Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# check whether we are in Google colab\n",
    "try:\n",
    "    from google.colab import files\n",
    "    print(\"running in COLAB mode\")\n",
    "    repo = 'https://github.com/mostly-ai/mostly-tutorials/raw/dev/multi-table'\n",
    "except:\n",
    "    print(\"running in LOCAL mode\")\n",
    "    repo = '.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import data into DB\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "csv_files = [\n",
    "    f'{repo}/account.csv', \n",
    "    f'{repo}/card.csv', \n",
    "    f'{repo}/client.csv', \n",
    "    f'{repo}/disposition.csv', \n",
    "    f'{repo}/district.csv',\n",
    "    f'{repo}/loan.csv', \n",
    "    f'{repo}/orders.csv', \n",
    "    f'{repo}/transaction.csv'\n",
    "]\n",
    "\n",
    "engine = connect_db(db_host, db_usr, db_pwd, db_name_source)\n",
    "\n",
    "originals = {}\n",
    "for fn in csv_files:\n",
    "    # read data from CSV into Pandas DataFrame\n",
    "    df = pd.read_csv(fn)\n",
    "    # ensure all columns are NULL-able\n",
    "    df = df.convert_dtypes()\n",
    "    # convert date columns\n",
    "    for col in df.columns:\n",
    "        if col in ['date', 'issued']:\n",
    "            df[col] = pd.to_datetime(df[col])\n",
    "        if col.endswith('_id'):\n",
    "            df[col] = df[col].astype(str)\n",
    "    # get filename w/o extension\n",
    "    db_table = Path(fn).stem\n",
    "    # write DataFrame to DB\n",
    "    df.to_sql(db_table, engine, index=False, if_exists='fail')\n",
    "    print(f\"created table `{db_table}` with {df.shape[0]:,} records\")\n",
    "    originals[db_table] = df\n",
    "\n",
    "print('DONE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with engine.connect() as conn:\n",
    "    # define primary keys in the database\n",
    "    conn.execute(text('ALTER TABLE account ADD PRIMARY KEY (account_id);'))\n",
    "    conn.execute(text('ALTER TABLE card ADD PRIMARY KEY (card_id);'))\n",
    "    conn.execute(text('ALTER TABLE client ADD PRIMARY KEY (client_id);'))\n",
    "    conn.execute(text('ALTER TABLE disposition ADD PRIMARY KEY (disposition_id);'))\n",
    "    conn.execute(text('ALTER TABLE district ADD PRIMARY KEY (district_id);'))\n",
    "    conn.execute(text('ALTER TABLE loan ADD PRIMARY KEY (loan_id);'))\n",
    "    conn.execute(text('ALTER TABLE orders ADD PRIMARY KEY (orders_id);'))\n",
    "    conn.execute(text('ALTER TABLE transaction ADD PRIMARY KEY (transaction_id);'))\n",
    "    print(f\"created primary keys\")\n",
    "    # define foreign key constraints in the database\n",
    "    conn.execute(text('ALTER TABLE account ADD CONSTRAINT fk_district_a FOREIGN KEY (district_id) REFERENCES district (district_id);')) #\n",
    "    conn.execute(text('ALTER TABLE client ADD CONSTRAINT fk_district_c FOREIGN KEY (district_id) REFERENCES district (district_id);')) #\n",
    "    conn.execute(text('ALTER TABLE disposition ADD CONSTRAINT fk_disp_a FOREIGN KEY (account_id) REFERENCES account (account_id);'))\n",
    "    conn.execute(text('ALTER TABLE disposition ADD CONSTRAINT fk_disp_c FOREIGN KEY (client_id) REFERENCES client (client_id);'))\n",
    "    conn.execute(text('ALTER TABLE card ADD CONSTRAINT fk_card FOREIGN KEY (disposition_id) REFERENCES disposition (disposition_id);'))\n",
    "    conn.execute(text('ALTER TABLE transaction ADD CONSTRAINT fk_trans FOREIGN KEY (account_id) REFERENCES account (account_id);'))\n",
    "    conn.execute(text('ALTER TABLE loan ADD CONSTRAINT fk_loan FOREIGN KEY (account_id) REFERENCES account (account_id);'))\n",
    "    conn.execute(text('ALTER TABLE orders ADD CONSTRAINT fk_order FOREIGN KEY (account_id) REFERENCES account (account_id);'))\n",
    "    print(f\"created foreign keys\")\n",
    "    conn.commit()\n",
    "print('DONE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oZ7ERZK__8TB",
    "tags": []
   },
   "source": [
    "## Synthesize Data via MOSTLY AI\n",
    "\n",
    "Go to MOSTLY AI, and\n",
    "\n",
    "1. Create two data connectors, one for the source DB `berka_original`, and one for the destination DB `berka_synthetic`\n",
    "\n",
    "<img src='https://raw.githubusercontent.com/mostly-ai/mostly-tutorials/dev/multi-table/mostly0a.png' width=\"400px\"/> <img src='https://raw.githubusercontent.com/mostly-ai/mostly-tutorials/dev/multi-table/mostly0b.png' width=\"400px\"/><br />\n",
    "\n",
    "2. Create a data catalog using the data connector for `berka_original`\n",
    "\n",
    "    - Select the `account` table along with all of its child tables\n",
    "    - Select the `client` table together with all of its child tables\n",
    "    - Configure smart select column `district_id` for the `disposition -> client` relation\n",
    "\n",
    "<img src='https://raw.githubusercontent.com/mostly-ai/mostly-tutorials/dev/multi-table/mostly1.png' width=\"400px\"/> <img src='https://raw.githubusercontent.com/mostly-ai/mostly-tutorials/dev/multi-table/mostly2.png' width=\"400px\"/><br />\n",
    "<img src='https://raw.githubusercontent.com/mostly-ai/mostly-tutorials/dev/multi-table/mostly3.png' width=\"400px\"/> <img src='https://raw.githubusercontent.com/mostly-ai/mostly-tutorials/dev/multi-table/mostly4.png' width=\"400px\"/><br />\n",
    "<img src='https://raw.githubusercontent.com/mostly-ai/mostly-tutorials/dev/multi-table/mostly5.png' width=\"400px\"/> <img src='https://raw.githubusercontent.com/mostly-ai/mostly-tutorials/dev/multi-table/mostly6.png' width=\"400px\"/><br />\n",
    "<img src='https://raw.githubusercontent.com/mostly-ai/mostly-tutorials/dev/multi-table/mostly7.png' width=\"400px\"/> \n",
    "\n",
    "These are then the configured table types and relations.\n",
    "\n",
    "<img src='https://raw.githubusercontent.com/mostly-ai/mostly-tutorials/dev/multi-table/berka-synthetic.png' width=\"600px\"/>\n",
    "\n",
    "3. Launch the job, and select `berka_synthetic` as a destination in \"Output settings\"\n",
    "\n",
    "4. Once the job has completed, continue with executing the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# fetch synthetic data from destination database\n",
    "engine = connect_db(db_host, db_usr, db_pwd, db_name_destination)\n",
    "tables = [Path(fn).stem for fn in csv_files if 'district' not in fn]\n",
    "synthetics = {}\n",
    "for db_table in tables:\n",
    "    with engine.begin() as conn:\n",
    "        df = pd.read_sql_query(sql=text(f'select * from {db_table};'), con=conn)\n",
    "    print(f\"extracted table {db_table} with {df.shape[0]:,} records\")\n",
    "    synthetics[db_table] = df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore Synthetic Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Show sample records for each table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for k in synthetics:\n",
    "    print(\"===\", k, \"===\")\n",
    "    display(synthetics[k].sample(n=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Check basic statistics\n",
    "\n",
    "The newly generated tables are statistically representative of the original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "display(synthetics['transaction']['amount'].quantile(q=[.1, .5, .9]))\n",
    "display(originals['transaction']['amount'].quantile(q=[.1, .5, .9]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "display(synthetics['account']['date'].quantile(q=[.1, .5, .9], interpolation='nearest'))\n",
    "display(pd.to_datetime(originals['account']['date']).quantile(q=[.1, .5, .9], interpolation='nearest'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Check referential integrity\n",
    "\n",
    "The newly generated foreign keys are also present as primary keys in the connected tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "assert synthetics['transaction']['account_id'].isin(synthetics['account']['account_id']).all()\n",
    "assert synthetics['card']['disposition_id'].isin(synthetics['disposition']['disposition_id']).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check context relations\n",
    "\n",
    "The cardinality of context FK relations is perfectly retained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('Orders per Account - Synthetic')\n",
    "display(synthetics['orders'].groupby('account_id').size().value_counts())\n",
    "print('\\nOrders per Account - Original')\n",
    "display(originals['orders'].groupby('account_id').size().value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('Cards per Disposition - Synthetic')\n",
    "display(synthetics['card'].groupby('disposition_id').size().value_counts())\n",
    "print('\\nCards per Disposition - Original')\n",
    "display(originals['card'].groupby('disposition_id').size().value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check smart select relations\n",
    "\n",
    "The cardinality of smart select FK relation is not retained, as these get randomly assigned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('\\nDispositions per Client - Synthetic')\n",
    "display(synthetics['disposition'].groupby('client_id').size().value_counts())\n",
    "print('Dispositions per Client - Original')\n",
    "display(originals['disposition'].groupby('client_id').size().value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the statistical relations between a child and its randomly assigned smart select parent can be retained, if corresponding smart select columns were configured. E.g. if smart select is properly configured, then the the share of cases where the `client` has the same `district_id` as the `account`, that she owns, should be similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def matching_districts(datasets):    \n",
    "    df = datasets['disposition']\n",
    "    df = df.loc[df.type=='OWNER']\n",
    "    df = df.merge(\n",
    "        datasets['client'], \n",
    "        on='client_id',\n",
    "    ).merge(\n",
    "        datasets['account'], \n",
    "        on='account_id',\n",
    "    )\n",
    "    return (df['district_id_x']==df['district_id_y']).mean()\n",
    "\n",
    "print(f\"Share of accounts and clients with identical district_id\")\n",
    "print(f\"synthetic: {matching_districts(synthetics):4.0%}\")\n",
    "print(f\"original:  {matching_districts(originals):4.0%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UMGNussThvys"
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "In this tutorial we have demonstrated how to synthesize a multi-table relational database. We have seen that structure, statistics and referential integrity are perfectly retained. We have also seen how to configure Smart Select, and its impact on retaining statistics across non-context relations. We also observed that there are limitations to what can be retained, in particular when it comes to the cardinality of smart select relations.\n",
    "\n",
    "If you are interested in learning more about how to run ad-hoc multi-table jobs that are not synced to a relational database, check out [the video tutorial]()."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References<a class=\"anchor\" name=\"refs\"></a>\n",
    "\n",
    "1. https://data.world/lpetrocelli/czech-financial-dataset-real-anonymized-transactions"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
