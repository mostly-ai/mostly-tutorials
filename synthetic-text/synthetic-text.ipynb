{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uUvsR-mWBoNS"
   },
   "source": [
    "# Synthetic Text Generation\n",
    "\n",
    "In this notebook, we demonstrate how to synthesize free text columns, and will furthermore explore its quality.\n",
    "\n",
    "For further background see also [this blog post](https://mostly.ai/blog/synthetic-data-for-text-annotation/) on \"How To Scale Up Your Text Annotation Initiatives with Synthetic Text\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MKRa93uuSqZS"
   },
   "source": [
    "## Synthesize Data via MOSTLY AI\n",
    "\n",
    "1. Download `london.csv` by clicking [here](https://github.com/mostly-ai/mostly-tutorials/raw/dev/synthetic-text/london.csv), and pressing Ctrl+S to save the file locally.\n",
    "\n",
    "2. Synthesize `london.csv` via [MOSTLY AI](https://mostly.ai/), and configure `host_name` and `title` as Encoding Type `Text` \n",
    "\n",
    "3. Once the job has finished, which might take up to 1 hour, download the generated synthetic data as CSV file to your computer.\n",
    "\n",
    "4. Upload the generated synthetic data to this Notebook via executing the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 89
    },
    "id": "W4IDNOIyPW7L",
    "outputId": "413106b9-de23-441b-ae27-f6efe45085a6",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# upload synthetic dataset\n",
    "import pandas as pd\n",
    "try:\n",
    "    # check whether we are in Google colab\n",
    "    from google.colab import files\n",
    "    print(\"running in COLAB mode\")\n",
    "    repo = 'https://github.com/mostly-ai/mostly-tutorials/raw/dev/synthetic-text'\n",
    "    import io\n",
    "    uploaded = files.upload()\n",
    "    syn = pd.read_csv(io.BytesIO(list(uploaded.values())[0]))\n",
    "    print(f\"uploaded synthetic data with {syn.shape[0]:,} records and {syn.shape[1]:,} attributes\")\n",
    "except:\n",
    "    print(\"running in LOCAL mode\")\n",
    "    repo = '.'\n",
    "    print(\"adapt `syn_file_path` to point to your generated synthetic data file\")\n",
    "    syn_file_path = './london-synthetic-representative.csv'\n",
    "    syn = pd.read_csv(syn_file_path)\n",
    "    print(f\"read synthetic data with {syn.shape[0]:,} records and {syn.shape[1]:,} attributes\")\n",
    "    \n",
    "tgt = pd.read_csv(f'{repo}/london.csv')\n",
    "print(f\"read original data with {tgt.shape[0]:,} records and {tgt.shape[1]:,} attributes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OgvJ0XoWTHoX"
   },
   "source": [
    "## Explore Synthetic Text\n",
    "\n",
    "Show 10 randomly sampled synthetic records. Note, that you can execute the following cell multiple times, to see different samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "syn.sample(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare this to 10 randomly sampled original records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tgt.sample(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Inspect Character Set\n",
    "\n",
    "You will note, that the character set of the synthetic data is shorter. This is due to the privacy mechanism within the MOSTLY AI platform, where very rare tokens are being removed, to prevent that their presence give away information on the existence of individual records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('## ORIGINAL ##\\n', ''.join(sorted(list(set(tgt['title'].str.cat(sep=' '))))), '\\n')\n",
    "print('## SYNTHETIC ##\\n', ''.join(sorted(list(set(syn['title'].str.cat(sep=' '))))), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect Character Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "title_char_freq = pd.merge(\n",
    "    tgt['title'].str.split('').explode().value_counts(normalize=True).to_frame('tgt').reset_index(),\n",
    "    syn['title'].str.split('').explode().value_counts(normalize=True).to_frame('syn').reset_index(),\n",
    "    on='index', \n",
    "    how='outer'\n",
    ").rename(columns={'index': 'char'}).round(5)\n",
    "title_char_freq.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "ax = title_char_freq.head(100).plot.line()\n",
    "plt.title('Distribution of Char Frequencies')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that Character Frequencies are perfectly retained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Inspect Term Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "def sanitize(s):\n",
    "    s = str(s).lower()\n",
    "    s = re.sub('[\\\\,\\\\.\\\\)\\\\(\\\\!\\\\\"\\\\:\\\\/]', ' ', s)\n",
    "    s = re.sub('[ ]+', ' ', s)\n",
    "    return s\n",
    "\n",
    "tgt['terms'] = tgt['title'].apply(lambda x: sanitize(x)).str.split(' ')\n",
    "syn['terms'] = syn['title'].apply(lambda x: sanitize(x)).str.split(' ')\n",
    "    \n",
    "title_term_freq = pd.merge(\n",
    "    tgt['terms'].explode().value_counts(normalize=True).to_frame('tgt').reset_index(),\n",
    "    syn['terms'].explode().value_counts(normalize=True).to_frame('syn').reset_index(),\n",
    "    on='index', \n",
    "    how='outer'\n",
    ").rename(columns={'index': 'term'}).round(5)\n",
    "display(title_term_freq.head(10))\n",
    "display(title_term_freq.head(200).tail(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ax = title_term_freq.head(100).plot.line()\n",
    "plt.title('Distribution of Term Frequencies')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that Term Frequencies are perfectly retained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Inspect Term Co-occurrence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calc_conditional_probability(term1, term2):\n",
    "    tgt_beds = tgt['title'][tgt['title'].str.lower().str.contains(term1).fillna(False)]\n",
    "    syn_beds = syn['title'][syn['title'].str.lower().str.contains(term1).fillna(False)]\n",
    "    tgt_beds_double = tgt_beds.str.lower().str.contains(term2).mean()\n",
    "    syn_beds_double = syn_beds.str.lower().str.contains(term2).mean()\n",
    "    print(f\"{tgt_beds_double:.0%} of actual Listings, that contain `{term1}`, also contain `{term2}`\")\n",
    "    print(f\"{syn_beds_double:.0%} of synthetic Listings, that contain `{term1}`, also contain `{term2}`\")\n",
    "    print(\"\")\n",
    "\n",
    "calc_conditional_probability('bed', 'double')\n",
    "calc_conditional_probability('bed', 'king')\n",
    "calc_conditional_probability('heart', 'london')\n",
    "calc_conditional_probability('london', 'heart')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that Term Co-occurrences are perfectly retained.\n",
    "\n",
    "Now you might be asking yourself: if all of these characteristics are maintained, what are the chances that we'll end up with exact matches, i.e. synthetic records with the exact same `title` value as a record in the original dataset? Or even a synthetic record with the exact same values for all the columns?\n",
    "\n",
    "Let's start by trying to find an exact match for 1 specific synthetic `title` value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find exact match for 1 specific synthetic title value. Copy a `title` value from a synthetic record into the `title_value` field below and run the cell to find an exact match in the original dataset\n",
    "title_value = \"Airy large double room\"\n",
    "tgt.loc[tgt['title'].str.contains(title_value, case=False, na=False)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on your chosen value, you may or may not find an exact match. This row-by-row validation process doesn't indicate very much and, more importantly, doesn't scale very well to the 71K rows in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Inspect Privacy via Exact Matches\n",
    "\n",
    "Let's perform a simplified check for privacy, by looking for exact matches between the synthetic and the original.\n",
    "\n",
    "For that we first split the original data into two equally-sized sets, and measure the number of matches between those two sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n = int(tgt.shape[0]/2)\n",
    "pd.merge(tgt[['title']][:n].drop_duplicates(), tgt[['title']][n:].drop_duplicates())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we take an equally-sized subset of the synthetic data, and again measure the number of matches between that set and the original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pd.merge(tgt[['title']][:n].drop_duplicates(), syn[['title']][:n].drop_duplicates())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that exact matches between original and synthetic data can occur. However, they occur only for the most commonly used descriptions, and they do not occur more often than they occur in the original data itself.\n",
    "\n",
    "Thus, it's important to note, that matchinig values or matching complete records are by themselves not a sign of privacy leak. They are only an issue if they occur more frequently than we would expect based on the original dataset. Also note that removing those exact matches via post-processing would have a detrimental contrary effect. The absence of a value like \"Lovely single room\" in a sufficiently large synthetic text corpus would in this case actually give away the fact that this sentence was present in the original. See [[1](#refs)] respectively [[2](#refs)] for more background info on this aspect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze Price vs. Text correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tgt_term_price = tgt[['terms', 'price']].explode(column='terms').groupby('terms')['price'].median()\n",
    "syn_term_price = syn[['terms', 'price']].explode(column='terms').groupby('terms')['price'].median()\n",
    "def print_term_price(term):\n",
    "    print(f\"Median Price of actual Listings, that contain `{term}`: ${tgt_term_price[term]:.0f}\")\n",
    "    print(f\"Median Price of synthetic Listings, that contain `{term}`: ${syn_term_price[term]:.0f}\")\n",
    "    print(\"\")\n",
    "\n",
    "print_term_price(\"luxury\")\n",
    "print_term_price(\"stylish\")\n",
    "print_term_price(\"cozy\")\n",
    "print_term_price(\"small\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that correlations between Term occurence and the price per night, are also perfectly retained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eQMOiU1Bv6W4"
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "This tutorial demonstrated how synthetic text can be generated wihtin the context of an otherwise structured dataset. We analyzed the generated texts, and validated that characters and terms occur with the same frequency, while exact matches do not occur anymore likely than within the actual text itself.\n",
    "\n",
    "This feature thus allows to retain valuable statistical insights, typically burried away in free text columns, that remain inaccessible due to their privacy sensitive nature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further exercises\n",
    "\n",
    "In addition to walking through the above instructions, we suggest..\n",
    "* analyzing further correlations, also for `host_name`\n",
    "* using a different generation mood, eg. conservative sampling\n",
    "* using a different dataset, eg. the Austrian First Name [[3](#refs)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UW5ntiUB18yP"
   },
   "source": [
    "## References<a class=\"anchor\" name=\"refs\"></a>\n",
    "\n",
    "1. https://github.com/mostly-ai/public-demo-data/blob/dev/firstnames_at/firstnames_at.csv.gz\n",
    "1. https://www.frontiersin.org/articles/10.3389/fdata.2021.679939/full\n",
    "1. https://mostly.ai/blog/truly-anonymous-synthetic-data-legal-definitions-part-ii/"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
