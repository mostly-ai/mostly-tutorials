{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3fd5150a-d33e-435b-ac47-3fdf15fb05c2",
   "metadata": {},
   "source": [
    "# Quality Assurance\n",
    "\n",
    "In this tutorial we will replicate key parts of the **accuracy** and **privacy** calculations of MOSTLY AI's Quality Assurance report. For a more extensive treatment on the topic, see also [our documentation](https://mostly.ai/synthetic-data-generator-docs/mostly-ai-help-support-tutorials-documentation), our [peer-reviewed journal paper](https://www.frontiersin.org/articles/10.3389/fdata.2021.679939/full), as well as the accompanying [benchmarking study](https://mostly.ai/blog/how-to-benchmark-synthetic-data-generators)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa3bd75-3d32-44e2-87d3-b9895753b27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pandas==2.0\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "print(f\"loaded NumPy {np.__version__}\")\n",
    "print(f\"loaded Pandas {pd.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08145f83-a985-4f4b-a02f-6f3ed3dbe6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo = 'https://github.com/mostly-ai/mostly-tutorials/raw/dev/quality-assurance'\n",
    "tgt = pd.read_parquet(f'{repo}/census-training.parquet')\n",
    "print(f'fetched original data with {tgt.shape[0]:,} records and {tgt.shape[1]} attributes')\n",
    "syn = pd.read_parquet(f'{repo}/census-synthetic.parquet')\n",
    "print(f'fetched synthetic data with {syn.shape[0]:,} records and {syn.shape[1]} attributes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ce2c48-89e8-4f87-bcfe-97be95e25212",
   "metadata": {},
   "outputs": [],
   "source": [
    "tgt.sample(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d955e83-d15d-4a74-a85a-55c60c248d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "syn.sample(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6e7026-494d-484d-b935-e366a4d695f4",
   "metadata": {},
   "source": [
    "## Accuracy Calculation\n",
    "\n",
    "Accuracy is measured as the distances between the lower-level (binned) marginal empirical distributions of the original vs. the synthetic dataset. We perform the calculation for all univariate and bivariate distributions, and then average across to determine simple summary statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c056183-1d3c-40a2-b528-90f058ce1d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bin_data(dt1, dt2, bins=10):\n",
    "    dt1 = dt1.copy()\n",
    "    dt2 = dt2.copy()\n",
    "    # quantile binning of numerics\n",
    "    num_cols = dt1.select_dtypes(include='number').columns\n",
    "    cat_cols = dt1.select_dtypes(include=['object', 'category', 'string', 'bool']).columns\n",
    "    for col in num_cols:\n",
    "        # determine breaks based on `dt1`\n",
    "        breaks = dt1[col].quantile(np.linspace(0, 1, bins+1)).unique()\n",
    "        dt1[col] = pd.cut(dt1[col], bins=breaks, include_lowest=True)\n",
    "        dt2_vals = pd.to_numeric(dt2[col], 'coerce')\n",
    "        dt2_bins = pd.cut(dt2_vals, bins=breaks, include_lowest=True)\n",
    "        dt2_bins[dt2_vals < min(breaks)] = '_other_'\n",
    "        dt2_bins[dt2_vals > max(breaks)] = '_other_'\n",
    "        dt2[col] = dt2_bins\n",
    "    # top-C binning of categoricals\n",
    "    for col in cat_cols:\n",
    "        dt1[col] = dt1[col].astype('str')\n",
    "        dt2[col] = dt2[col].astype('str')\n",
    "        # determine top values based on `dt1`\n",
    "        top_vals = dt1[col].value_counts().head(bins).index.tolist()\n",
    "        dt1[col].replace(np.setdiff1d(dt1[col].unique().tolist(), top_vals), '_other_', inplace=True)\n",
    "        dt2[col].replace(np.setdiff1d(dt2[col].unique().tolist(), top_vals), '_other_', inplace=True)\n",
    "    return dt1, dt2\n",
    "\n",
    "    \n",
    "def calculate_accuracies(dt1_bin, dt2_bin, k=1):\n",
    "    # build grid of all cross-combinations\n",
    "    cols = dt1_bin.columns\n",
    "    interactions = pd.DataFrame(np.array(np.meshgrid(cols, cols)).reshape(2, len(cols)**2).T)\n",
    "    interactions.columns = ['col1', 'col2']\n",
    "    if k == 1:\n",
    "        interactions = interactions.loc[(interactions['col1']==interactions['col2'])]\n",
    "    elif k == 2:\n",
    "        interactions = interactions.loc[(interactions['col1']<interactions['col2'])]\n",
    "    else:\n",
    "        raise('k>2 not supported')\n",
    "\n",
    "    results = []\n",
    "    for idx in range(interactions.shape[0]):\n",
    "        row = interactions.iloc[idx]\n",
    "        val1 = dt1_bin[row.col1].astype(str) + \"|\" + dt1_bin[row.col2].astype(str)\n",
    "        val2 = dt2_bin[row.col1].astype(str) + \"|\" + dt2_bin[row.col2].astype(str)\n",
    "        # calculate empirical marginal distributions (=relative frequencies)\n",
    "        freq1 = val1.value_counts(normalize=True, dropna=False).to_frame(name='p1')\n",
    "        freq2 = val2.value_counts(normalize=True, dropna=False).to_frame(name='p2')\n",
    "        freq = freq1.join(freq2, how='outer').fillna(0.0)\n",
    "        # calculate Total Variation Distance between relative frequencies\n",
    "        tvd = np.sum(np.abs(freq['p1'] - freq['p2'])) / 2\n",
    "        # calculate Accuracy as (100% - TVD)\n",
    "        acc = (1 - tvd)\n",
    "        out = pd.DataFrame({\n",
    "          'Column': [row.col1], 'Column 2': [row.col2],\n",
    "          'TVD': [tvd], 'Accuracy': [acc],\n",
    "        })\n",
    "        results.append(out)\n",
    "\n",
    "    return pd.concat(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1131d20-9aeb-49e2-a367-75ebca3a43b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# restrict to max 100k records\n",
    "tgt = tgt.sample(frac=1).head(n=100_000)\n",
    "syn = syn.sample(frac=1).head(n=100_000)\n",
    "# bin data\n",
    "tgt_bin, syn_bin = bin_data(tgt, syn, bins=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da31719-e0ae-4553-b12b-824e8917acab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate univariate accuracies\n",
    "acc_uni = calculate_accuracies(tgt_bin, syn_bin, k=1)[['Column', 'Accuracy']]\n",
    "acc_uni.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3b6a10-b1f5-4c13-8563-3722d1fa1bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate bivariate accuracies\n",
    "acc_biv = calculate_accuracies(tgt_bin, syn_bin, k=2)[['Column', 'Column 2', 'Accuracy']]\n",
    "acc_biv = pd.concat([acc_biv, acc_biv.rename(columns={'Column': 'Column 2', 'Column 2': 'Column'})])\n",
    "acc_biv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8894d311-9ab6-44ca-affa-3eee143d87d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the average bivariate accuracy\n",
    "acc_biv_avg = acc_biv.groupby('Column')['Accuracy'].mean().to_frame('Bivariate Accuracy').reset_index()\n",
    "# merge to univariate and avg. bivariate accuracy to single overview table\n",
    "acc = pd.merge(acc_uni.rename(columns={'Accuracy': 'Univariate Accuracy'}), acc_biv_avg, on='Column').sort_values('Univariate Accuracy', ascending=False)\n",
    "# report accuracy as percentage\n",
    "acc['Univariate Accuracy'] = acc['Univariate Accuracy'].apply(lambda x: f\"{x:.1%}\")\n",
    "acc['Bivariate Accuracy'] = acc['Bivariate Accuracy'].apply(lambda x: f\"{x:.1%}\")\n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb0bf41-e98e-4984-9c19-d9e05f539a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Avg. Univariate Accuracy: {acc_uni['Accuracy'].mean():.1%}\")\n",
    "print(f\"Avg. Bivariate Accuracy:  {acc_biv['Accuracy'].mean():.1%}\")\n",
    "print(f\"-------------------------------\")\n",
    "acc_avg = (acc_uni['Accuracy'].mean() + acc_biv['Accuracy'].mean()) / 2\n",
    "print(f\"Avg. Overall Accuracy:    {acc_avg:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc18856b-0247-483d-8e6a-0c48d5089ed8",
   "metadata": {},
   "source": [
    "## Accuracy Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdacb16d-cff4-427e-9dea-9619e38c3110",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "def plot_univariate(tgt_bin, syn_bin, col, accuracy):\n",
    "    freq1 = tgt_bin[col].value_counts(normalize=True, dropna=False).to_frame('tgt')\n",
    "    freq2 = syn_bin[col].value_counts(normalize=True, dropna=False).to_frame('syn')\n",
    "    freq = freq1.join(freq2, how='outer').fillna(0.0).reset_index()\n",
    "    freq = freq.sort_values(col)\n",
    "    freq[col] = freq[col].astype(str)\n",
    "    \n",
    "    layout = go.Layout(\n",
    "        title=dict(text=f\"<b>{col}</b> <sup>{accuracy:.1%}</sup>\", x=0.5, y=0.98),\n",
    "        autosize=True,\n",
    "        height=300,\n",
    "        width=800,\n",
    "        margin=dict(l=10, r=10, b=10, t=40, pad=5),\n",
    "        plot_bgcolor=\"#eeeeee\",\n",
    "        hovermode=\"x unified\",\n",
    "        yaxis=dict(\n",
    "            zerolinecolor=\"white\",\n",
    "            rangemode=\"tozero\",\n",
    "            tickformat=\".0%\",\n",
    "        ),\n",
    "    )\n",
    "    fig = go.Figure(layout=layout)\n",
    "    trn_line = go.Scatter(\n",
    "        mode=\"lines\",\n",
    "        x=freq[col],\n",
    "        y=freq[\"tgt\"],\n",
    "        name=\"target\",\n",
    "        line_color=\"#666666\",\n",
    "        yhoverformat=\".2%\",\n",
    "    )\n",
    "    syn_line = go.Scatter(\n",
    "        mode=\"lines\",\n",
    "        x=freq[col],\n",
    "        y=freq[\"syn\"],\n",
    "        name=\"synthetic\",\n",
    "        line_color=\"#24db96\",\n",
    "        yhoverformat=\".2%\",\n",
    "        fill=\"tonexty\",\n",
    "        fillcolor=\"#ffeded\",\n",
    "    )\n",
    "    fig.add_trace(trn_line)\n",
    "    fig.add_trace(syn_line)\n",
    "    fig.show(config= dict(displayModeBar = False))\n",
    "\n",
    "def plot_bivariate(tgt_bin, syn_bin, col1, col2, accuracy):\n",
    "    x = pd.concat([tgt_bin[col1], syn_bin[col1]]).drop_duplicates().to_frame(col1)\n",
    "    y = pd.concat([tgt_bin[col2], syn_bin[col2]]).drop_duplicates().to_frame(col2)\n",
    "    df = pd.merge(x, y, how=\"cross\")\n",
    "    df = pd.merge(\n",
    "        df,\n",
    "        pd.concat([tgt_bin[col1], tgt_bin[col2]], axis=1)\n",
    "        .value_counts()\n",
    "        .to_frame(\"target\")\n",
    "        .reset_index(),\n",
    "        how=\"left\",\n",
    "    )\n",
    "    df = pd.merge(\n",
    "        df,\n",
    "        pd.concat([syn_bin[col1], syn_bin[col2]], axis=1)\n",
    "        .value_counts()\n",
    "        .to_frame(\"synthetic\")\n",
    "        .reset_index(),\n",
    "        how=\"left\",\n",
    "    )\n",
    "    df = df.sort_values([col1, col2], ascending=[True, True]).reset_index(drop=True)\n",
    "    df[\"target\"] = df[\"target\"].fillna(0.0)\n",
    "    df[\"synthetic\"] = df[\"synthetic\"].fillna(0.0)\n",
    "    # normalize values row-wise (used for visualization)\n",
    "    df[\"target_by_row\"] = df[\"target\"] / df.groupby(col1)[\"target\"].transform(\"sum\")\n",
    "    df[\"synthetic_by_row\"] = df[\"synthetic\"] / df.groupby(col1)[\"synthetic\"].transform(\"sum\")\n",
    "    # normalize values across table (used for accuracy)\n",
    "    df[\"target_by_all\"] = df[\"target\"] / df[\"target\"].sum()\n",
    "    df[\"synthetic_by_all\"] = df[\"synthetic\"] / df[\"synthetic\"].sum()\n",
    "    df[\"y\"] = df[col1].astype(\"str\")\n",
    "    df[\"x\"] = df[col2].astype(\"str\")\n",
    "\n",
    "    layout = go.Layout(\n",
    "        title=dict(text=f\"<b>{col1} ~ {col2}</b> <sup>{accuracy:.1%}</sup>\", x=0.5, y=0.98),\n",
    "        autosize=True,\n",
    "        height=300,\n",
    "        width=800,\n",
    "        margin=dict(l=10, r=10, b=10, t=40, pad=5),\n",
    "        plot_bgcolor=\"#eeeeee\",\n",
    "        showlegend=True,\n",
    "        # prevent Plotly from trying to convert strings to dates\n",
    "        xaxis=dict(type=\"category\"),\n",
    "        xaxis2=dict(type=\"category\"),\n",
    "        yaxis=dict(type=\"category\"),\n",
    "        yaxis2=dict(type=\"category\"),\n",
    "    )\n",
    "    fig = go.Figure(layout=layout).set_subplots(\n",
    "        rows=1,\n",
    "        cols=2,\n",
    "        horizontal_spacing=0.05,\n",
    "        shared_yaxes=True,\n",
    "        subplot_titles=(\"target\", \"synthetic\"),\n",
    "    )\n",
    "    fig.update_annotations(font_size=12)\n",
    "    # plot content\n",
    "    hovertemplate = (\n",
    "        col1[:10] + \": `%{y}`<br />\" + col2[:10] + \": `%{x}`<br /><br />\"\n",
    "    )\n",
    "    hovertemplate += \"share target vs. synthetic<br />\"\n",
    "    hovertemplate += \"row-wise: %{customdata[0]} vs. %{customdata[1]}<br />\"\n",
    "    hovertemplate += \"absolute: %{customdata[2]} vs. %{customdata[3]}<br />\"\n",
    "    customdata = df[\n",
    "        [\"target_by_row\", \"synthetic_by_row\", \"target_by_all\", \"synthetic_by_all\"]\n",
    "    ].apply(lambda x: x.map(\"{:.2%}\".format))\n",
    "    heat1 = go.Heatmap(\n",
    "        x=df[\"x\"],\n",
    "        y=df[\"y\"],\n",
    "        z=df[\"target_by_row\"],\n",
    "        name=\"target\",\n",
    "        zmin=0,\n",
    "        zmax=1,\n",
    "        autocolorscale=False,\n",
    "        colorscale=[\"white\", \"#A7A7A7\", \"#7B7B7B\", \"#666666\"],\n",
    "        showscale=False,\n",
    "        customdata=customdata,\n",
    "        hovertemplate=hovertemplate,\n",
    "    )\n",
    "    heat2 = go.Heatmap(\n",
    "        x=df[\"x\"],\n",
    "        y=df[\"y\"],\n",
    "        z=df[\"synthetic_by_row\"],\n",
    "        name=\"synthetic\",\n",
    "        zmin=0,\n",
    "        zmax=1,\n",
    "        autocolorscale=False,\n",
    "        colorscale=[\"white\", \"#81EAC3\", \"#43E0A5\", \"#24DB96\"],\n",
    "        showscale=False,\n",
    "        customdata=customdata,\n",
    "        hovertemplate=hovertemplate,\n",
    "    )\n",
    "    fig.add_trace(heat1, row=1, col=1)\n",
    "    fig.add_trace(heat2, row=1, col=2)\n",
    "    fig.show(config= dict(displayModeBar = False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b83c4a-bcfc-420e-9eee-88d015067898",
   "metadata": {},
   "source": [
    "### Univariate Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4027e0-e561-4e26-a38f-ead65b8b2a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot all empirical univariate distributions, and their accuracy\n",
    "for idx, row in acc_uni.sample(n=5, random_state=0).iterrows():\n",
    "    plot_univariate(tgt_bin, syn_bin, row['Column'], row['Accuracy'])\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd0ec07-d66d-4bfe-8c41-a359c2fe982f",
   "metadata": {},
   "source": [
    "### Bivariate Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04efb6b1-364f-4d5f-aa8a-4092b1173a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, row in acc_biv.sample(n=5, random_state=0).iterrows():\n",
    "    plot_bivariate(tgt_bin, syn_bin, row['Column'], row['Column 2'], row['Accuracy'])\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a5d4343-d05a-4575-9bb6-1c5d29b86417",
   "metadata": {},
   "source": [
    "## Privacy Calculation\n",
    "\n",
    "To gauge the privacy risk of the generated synthetic data, we calculate the distances between the synthetic samples and their \"nearest neighbor\", i.e., their most similar record, from the original dataset. We then compare these distances to the same distances calculated for the original dataset itself. We expect that the synthetic samples are not systematically any closer to the original, than the original samples are to each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d25d98d0-f378-4d7f-8cb1-dc269de2ddf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "no_of_records = min(tgt.shape[0] // 2, syn.shape[0], 10_000)\n",
    "tgt = tgt.sample(n=2 * no_of_records)\n",
    "trn = tgt.head(no_of_records)\n",
    "hol = tgt.tail(no_of_records)\n",
    "syn = syn.sample(n=no_of_records)\n",
    "\n",
    "string_cols = trn.select_dtypes(exclude=np.number).columns\n",
    "numeric_cols = trn.select_dtypes(include=np.number).columns\n",
    "transformer = make_column_transformer(\n",
    "    (SimpleImputer(missing_values=np.nan, strategy='mean'), numeric_cols),\n",
    "    (OneHotEncoder(), string_cols),\n",
    "    remainder=\"passthrough\",\n",
    ")\n",
    "transformer.fit(pd.concat([trn, hol, syn], axis=0))\n",
    "trn_hot = transformer.transform(trn)\n",
    "hol_hot = transformer.transform(hol)\n",
    "syn_hot = transformer.transform(syn)\n",
    "\n",
    "# calculcate distances to nearest neighbors\n",
    "index = NearestNeighbors(n_neighbors=2, algorithm=\"brute\", metric=\"l2\", n_jobs=-1)\n",
    "index.fit(trn_hot)\n",
    "# k-nearest-neighbor search for both training and synthetic data, k=2 to calculate DCR + NNDR\n",
    "dcrs_hol, _ = index.kneighbors(hol_hot)\n",
    "dcrs_syn, _ = index.kneighbors(syn_hot)\n",
    "dcrs_hol = np.square(dcrs_hol)\n",
    "dcrs_syn = np.square(dcrs_syn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a291b9c7-be7c-4181-bd73-628ce9e4c6a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dcr_bound = np.maximum(np.quantile(dcrs_hol[:, 0], 0.95), 1e-8)\n",
    "ndcr_hol = dcrs_hol[:,0]/dcr_bound\n",
    "ndcr_syn = dcrs_syn[:,0]/dcr_bound\n",
    "print(f\"Normalized DCR 5-th percentile original  {np.percentile(ndcr_hol, 5):.3f}\")\n",
    "print(f\"Normalized DCR 5-th percentile synthetic {np.percentile(ndcr_syn, 5):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df4726a-f227-4070-8b1c-218769c3672f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"NNDR 5-th percentile original  {np.percentile(dcrs_hol[:,0]/dcrs_hol[:,1], 5):.3f}\")\n",
    "print(f\"NNDR 5-th percentile synthetic {np.percentile(dcrs_syn[:,0]/dcrs_syn[:,1], 5):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf944e4f-5b10-49ff-b67b-c6386a9e30da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
